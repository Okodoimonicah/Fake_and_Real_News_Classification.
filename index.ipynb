{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Fake And Real News Classification**\n",
    "\n",
    "---\n",
    "\n",
    "Authors: [Femi Kamau](https://www.github.com/ctrl-Karugu), [Monicah Iwagit](), [Teofilo Gafna](), [Wendy Mwiti](https://www.github.com/WendyMwiti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Determine Business Objectives\n",
    "## 1.1.1 Background\n",
    "In recent decades, the prevalence of fake news in the media has gradually increased. The term \"fake news\" refers to news, information, or stories that are either wholly made up or inaccurate to some degree and are created to either influence peopleâ€™s views, push a political agenda or cause confusion. \n",
    "\n",
    "According to a survey carried out by Edelman Public Relations Firm, media trust has consistently dropped by 8% every year. There exists a general lack of confidence in whatever information is disseminated by the media with a percentage as high as 41% of people actively saying that they avoid the news.\n",
    "\n",
    "In the past, we relied on reliable journalists, media organizations, and sources who were bound by stringent ethical standards. However, the internet has made it possible to publish, exchange, and consume news and information in a completely new way with hardly any restrictions or editorial guidelines.\n",
    "\n",
    "Nowadays, a lot of people obtain their news from social media networks and websites, and it can frequently be challenging to determine whether a story is legitimate or not. Any surge in false news or hoax stories has also been attributed to information overload and a general lack of understanding by individuals of how the internet functions.\n",
    "Hence the need for our project -the building of an NLP based classifier that categorizes articles on  whether they are real or fake. \n",
    "\n",
    "## 1.1.2 Problem Statement\n",
    "Fake news has various effects on human life, some of which are surprising and could vary from the way we humans perceive risk, the content of our dreams and even to our likelihood of getting a health complication. Therefore, this project, in a bid to reduce some of these effects, is concerned with identifying a solution that could be used to detect and filter out articles containing fake news as it will prove  useful to both readers and companies (stakeholders of the project) interested in the issue.\n",
    "\n",
    "## 1.1.3 Research Questions\n",
    "* Which periods of the year have the most fake news?\n",
    "* Which person appears most in fake news?\n",
    "* What topics are most prevalent in fake news?\n",
    "* What are the most common keywords in fake news?\n",
    "\n",
    "## 1.1.4 Business Objectives\n",
    "* To establish which months have the most fake news.\n",
    "* To ascertain which subject dominated the fake news.\n",
    "* To  predict if news is fake or real\n",
    "\n",
    "## 1.1.5 Business Success Criteria\n",
    "Build an NLP Classification model that predicts, with an accuracy of 90%, the validity of the news articles and a precision of 90% \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 22:12:46.817297: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
      "2022-12-03 22:12:46.817362: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-03 22:12:52.603188: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2022-12-03 22:12:52.603236: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-03 22:12:52.611408: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-UT5CAP7\n",
      "2022-12-03 22:12:52.611720: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-UT5CAP7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.24.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (20.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.23.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.50.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from click<9.0.0,>=7.1.1->typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.3)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.4.1\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied, skipping upgrade: colorama; platform_system == \"Windows\" in c:\\users\\wendy\\anaconda3\\envs\\learn-env\\lib\\site-packages (from click) (0.4.3)\n",
      "Installing collected packages: click\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 7.1.2\n",
      "    Uninstalling click-7.1.2:\n",
      "      Successfully uninstalled click-7.1.2\n",
      "Successfully installed click-8.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install click --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score, auc, RocCurveDisplay\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, mean_absolute_error, mean_squared_error\n",
    "import pickle\n",
    "import spacy\n",
    "import unidecode\n",
    "from word2number import w2n\n",
    "import contractions\n",
    "import re\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Remove 'no' and 'not' from SpaCy's stop words list\n",
    "deselect_stop_words = ['no', 'not']\n",
    "for w in deselect_stop_words:\n",
    "    nlp.vocab[w].is_stop = False\n",
    "    \n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had 2 csv files sourced from kaggle which contains nearly 23481 fake news and 21417 real news posted between 2015-2018 from news sites such as 21st Century Wire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/True.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-e8d481eec6bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load the real news data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mreal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/True.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Load the fake news data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/Fake.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/True.csv'"
     ]
    }
   ],
   "source": [
    "# Load the real news data\n",
    "real = pd.read_csv('./data/True.csv')\n",
    "\n",
    "# Load the fake news data\n",
    "fake = pd.read_csv('./data/Fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview real data\n",
    "real.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the fake data\n",
    "fake.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Add a column called 'category' to both DataFrames which will become our taget variable\n",
    "real['category'] = 1\n",
    "fake['category'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine both DataFrames \n",
    "data = pd.concat([real, fake])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the new DataFrame head\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the new DataFrame tail\n",
    "data.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of columns in the file:\n",
    "\n",
    "`title`- contains news headlines\n",
    "\n",
    "`text`- contains news content/article\n",
    "\n",
    "`subject`- the type of news\n",
    "\n",
    "`date`- the date the news was published\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataUnderstanding(object):\n",
    "    \"\"\"A class that does basic Data Understanding\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.shape = df.shape\n",
    "        self.info = df.info\n",
    "        self.duplicates = df.duplicated().sum()\n",
    "        self.missing = df.isna().sum()\n",
    "        self.types = df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-14fa18b5309c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Instantiate the class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0munderstanding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataUnderstanding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Instantiate the class\n",
    "understanding = dataUnderstanding(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the dataset\n",
    "print(f\"Shape:{understanding.shape}\")\n",
    "print()\n",
    "print(understanding.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary above, we can see that the dataset contain 44898 rows and spans 5 columns. The columns are: title, text, subject, date and category. The category column is the target variable and the rest are the features.\n",
    "\n",
    "Furthermore, the dataset contains 4 object columns and 1 integer column. The object columns are: title, text, subject and date. The integer column is: category (target variable). We may need to convert the date column to datetime format in the data preparation phase.\n",
    "\n",
    "The dataset does not contain any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(f\"Duplicates: {understanding.duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 209 duplicates. They shall be inspected and removed if necessary in the data preparation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'understanding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-4639b26df3fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Check the number of missing values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0munderstanding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'understanding' is not defined"
     ]
    }
   ],
   "source": [
    "# Check the number of missing values\n",
    "understanding.missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the value counts in the subject column\n",
    "data['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The articles that are within this dataset fall under 8 different subjects. These are: politicsNews, worldnews, News, politics, left-news, Government News, US_News, and Middle-east."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the value counts in the category column\n",
    "data['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is fairly balanced between fake and real news "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the date column\n",
    "data['date'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date column contains 2397 unique dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To ensure validity within the dataset, we will be checking that the data is in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the subject column to category\n",
    "data['subject'] = data['subject'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting date from object to datetime\n",
    "data['date'] = pd.to_datetime(data['date'], dayfirst=True, errors='coerce')\n",
    "\n",
    "# Type of the date column\n",
    "data['date'].dtype\n",
    "\n",
    "# Preview the updated DataFrame \n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the months and the years from the date column\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.strftime('%B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the updated datatypes\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this section, we will be obeserving the consistency of the data. We will be checking for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dataframe for the duplicated to be inspected\n",
    "duplicates  = data[data.duplicated()]\n",
    "duplicates.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicates\n",
    "print(f\"Before dropping: {len(data)}\")\n",
    "data.drop_duplicates(inplace=True)\n",
    "print(f\"After dropping: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the URLS from the text column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From data understanding section we found there to be no missing values. Therefore we can confirm that the the dataset is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Uniformity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To check on whether different systems refer to the same value in the same format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the value counts of the subject column\n",
    "data['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the categories in the subject column\n",
    "data['subject'].replace({'politicsNews': 'politics',\n",
    "                         'worldnews': 'world_news',\n",
    "                         'News': 'news',\n",
    "                         'left-news': 'left_news',\n",
    "                         'Government News': 'government_news',\n",
    "                         'US_News': 'us_news',\n",
    "                         'Middle-east': 'middle_east'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the updated subject column value counts\n",
    "data['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1 Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that visualizes the value counts of a column\n",
    "def plot_bar(df, col)-> None:\n",
    "    \"\"\" A function that returns a plot count of columns\"\"\"\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.countplot(data=df, x = col, order=df[col].value_counts().index)\n",
    "    plt.title(f\"{col} count plot\", fontsize=25)\n",
    "    plt.ylabel(\"Count\", fontsize=15)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xlabel(f\"{col}\", fontsize=15)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.1.1 `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_cloud(df, target, feature, i:int):\n",
    "    \"\"\"This fuction creates a wordcloud for the news texts\"\"\"\n",
    "    real_news =df[df[target]==i]\n",
    "    text = data[feature].values\n",
    "    wordcloud = WordCloud(\n",
    "        max_words = 400,\n",
    "        width = 800,\n",
    "        height = 600,\n",
    "        background_color = 'black',\n",
    "        stopwords = STOPWORDS).generate(str(text))\n",
    "    fig = plt.figure(\n",
    "        figsize = (14, 8),\n",
    "        facecolor = 'k',\n",
    "        edgecolor = 'k')\n",
    "    plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fake News Visualization Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_cloud(data, 'category', 'text', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Real News Visualization Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_cloud(data, 'category', 'text', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.1.2 `subject`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a countplot of the subject\n",
    "plot_bar(data, 'subject')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Most of the published news talk about politics followed by world news while the least discussed subject is the Middle East"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.1.3 `month`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a countplot of the Months\n",
    "plot_bar(data, 'month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Most of the news is published during the month of November followed by October and September.\n",
    "* It is also worth noting that the months of September and October precede November.\n",
    "* The month of August registered the least number of published news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.1.4 `year`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the years\n",
    "plot_bar(data, 'year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* 2017 had the most news followed by 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.1.5 `category`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a pie chart for the column 'category'\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "data['category'].value_counts().plot(kind='pie', autopct='%.0f%%');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* The data set is fairly balanced. However, fake news is slightly more than real news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2 Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that plots a count plot with respect to another column\n",
    "def plot_bivariate(df, col, by):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.countplot(data=df, x=col, hue=by)\n",
    "    plt.title(f\"{col} count plot by {by}\", fontsize=25)\n",
    "    plt.ylabel(\"count\", fontsize=15)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xlabel(f\"{col}\", fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.2.1 `month` & `category`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting month by category\n",
    "plot_bivariate(data, 'month', 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "  * From the visualization above, we see a spike in real news towards the end of the year. However, fake news remains constant throughout the year. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.2.2 `year` & `category`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot year by category\n",
    "plot_bivariate(data, 'year', 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* The visualization above tells us that majority of the news captured in this dataset is from the year 2016 and 2017. Furthermore, 2017 seems to have a higher amount of data output compared to the rest. This could be attributed to the fact that 2017 was the beginning of a new presidential term in the United States which always brings with it a fair share of news coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.2.3 `subject` & `category`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bivariate(data, 'subject', 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "* We see that the real news within this dataset falls under the politics nad world_news subjects. Therefore, a prediction system would probably work best for the aforementioned subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to remove web tags\n",
    "def remove_web_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    # Remove https links\n",
    "    clean = re.compile(r'https\\S*')\n",
    "    text = re.sub(clean, '', text)\n",
    "\n",
    "    # Remove data '.com' links\n",
    "    clean = re.compile(r'\\S+\\.com\\S+')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "\n",
    "# Function to remove twitter handles\n",
    "def remove_twitter_handles(text):\n",
    "    \"\"\"This function removes twitter handles from a string\"\"\"\n",
    "    clean = re.compile(r'@\\S*')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "\n",
    "# Function to convert Non-ASCII characters to ASCII\n",
    "def remove_accented_chars(text):\n",
    "    \"\"\"This function removes accented characters from text, e.g. cafÃ©\"\"\"\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Function to expand contractions\n",
    "def expand_contractions(text):\n",
    "    \"\"\"Expand shortened words, e.g. don't to do not\"\"\"\n",
    "    text = contractions.fix(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Function to remove special characters\n",
    "def remove_special_characters(text):\n",
    "    \"\"\"This function removes special characters from text, e.g. $\"\"\"\n",
    "    clean = re.compile(r'[^a-zA-Z0-9\\s]')\n",
    "    return re.sub(clean, ' ', text)\n",
    "\n",
    "\n",
    "# Function to lowercase text\n",
    "def lowercase_text(text):\n",
    "    \"\"\"This function converts characters to lowercase\"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "# Function to convert number words to digits\n",
    "def convert_number_words(text):\n",
    "    \"\"\"Convert number words to digits and remove them\"\"\"\n",
    "    \n",
    "    pattern = r'(\\W+)'\n",
    "    tokens = re.split(pattern, text)\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        try:\n",
    "            tokens[i] = str(w2n.word_to_num(token))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return ''.join(tokens)\n",
    "\n",
    "\n",
    "# Function to remove numbers\n",
    "def remove_numbers(text):\n",
    "    \"\"\"This function removes numbers from text\"\"\"\n",
    "    clean = re.compile(r'\\d+')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "\n",
    "# Function to remove short words\n",
    "def remove_small_words(text):\n",
    "    \"\"\"This function removes words with length 1 or 2\"\"\"\n",
    "    clean = re.compile(r'\\b\\w{1,2}\\b')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "\n",
    "# Function to remove names of people\n",
    "def remove_names(text):\n",
    "    \"\"\"This is a function that removes the names from text\"\"\"\n",
    "    with open('./data_preprocessing/names.txt', 'r') as f:\n",
    "        NAMES = set(f.read().splitlines())\n",
    "\n",
    "        NAMES = [name.lower() for name in NAMES]\n",
    "        \n",
    "    pattern = r'\\W+'\n",
    "    tokens = re.split(pattern, text)\n",
    "    \n",
    "    words = tokens\n",
    "      \n",
    "    for token in tokens:\n",
    "        if token in NAMES:\n",
    "            while token in words:\n",
    "                words.remove(token)\n",
    "    \n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Function to remove countries\n",
    "def remove_countries(text):\n",
    "    \"\"\"This is a function that removes the countries from text\"\"\"\n",
    "    with open('./data_preprocessing/countries.txt', 'r') as f:\n",
    "        COUNTRIES = set(f.read().splitlines())\n",
    "\n",
    "        COUNTRIES = [name.lower() for name in COUNTRIES]\n",
    "        \n",
    "    pattern = r'(\\W+)'\n",
    "    tokens = re.split(pattern, text)\n",
    "    \n",
    "    words = tokens\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in COUNTRIES:\n",
    "            while token in words:\n",
    "                words.remove(token)\n",
    "    \n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Function to remove US cities of people\n",
    "def remove_cities(text):\n",
    "    \"\"\"This is a function that removes the US cities from text\"\"\"\n",
    "    with open('./data_preprocessing/cities.txt', 'r') as f:\n",
    "        CITIES = set(f.read().splitlines())\n",
    "\n",
    "        CITIES = [name.lower() for name in CITIES]\n",
    "        \n",
    "    pattern = r'(\\W+)'\n",
    "    tokens = re.split(pattern, text)\n",
    "    \n",
    "    words = tokens\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in CITIES:\n",
    "            while token in words:\n",
    "                words.remove(token)\n",
    "    \n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Function to remove days and months\n",
    "def remove_days_and_months(text):\n",
    "    \"\"\"This is a function that removes the months and years from text\"\"\"\n",
    "    \n",
    "    # Load the months\n",
    "    with open('./data_preprocessing/months.txt', 'r') as f:\n",
    "        MONTHS = set(f.read().splitlines())\n",
    "\n",
    "        MONTHS = [name.lower() for name in MONTHS]\n",
    "    \n",
    "    # Load the days of the week\n",
    "    with open('./data_preprocessing/week.txt', 'r') as f:\n",
    "        WEEK = set(f.read().splitlines())\n",
    "\n",
    "        WEEK = [name.lower() for name in WEEK]\n",
    "      \n",
    "    pattern = r'(\\W+)'\n",
    "    tokens = re.split(pattern, text)  \n",
    "    \n",
    "    words = tokens\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in MONTHS:\n",
    "            while token in words:\n",
    "                words.remove(token)\n",
    "     \n",
    "    for token in tokens:\n",
    "        if token in WEEK:\n",
    "            while token in words:\n",
    "                words.remove(token)\n",
    "            \n",
    "    text = ' '.join(words)\n",
    "            \n",
    "    return text\n",
    "\n",
    "\n",
    "def stopwords(text):\n",
    "    \"\"\"This function removes the stopwords in the text\"\"\"\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords = set(stopwords)\n",
    "    \n",
    "    tokens = re.split(r'(\\W+)', text)\n",
    "    \n",
    "    text = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "    return ' '.join(text)\n",
    "\n",
    "\n",
    "# Function to remove extra spaces\n",
    "def remove_whitespace(text):\n",
    "    \"\"\"Remove extra spaces from a string\"\"\"\n",
    "    \n",
    "    clean = re.compile(r'\\s{2,10000}')\n",
    "    text = re.sub(clean, ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Lemmatize text\n",
    "def lemmatize(text):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    \n",
    "    tokens = re.split('\\W+', text)\n",
    "    \n",
    "    text = [lemma.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(text)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1070f7a72b9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Remove the web tags in the text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremove_web_tags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Remove the web tags in the text\n",
    "data['text'] = data['text'].apply(remove_web_tags)\n",
    "\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the twitter handles from the text\n",
    "data['text'] = data['text'].apply(remove_twitter_handles)\n",
    "\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the accented characters\n",
    "data['text'] = data['text'].apply(remove_accented_chars)\n",
    "\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the contractions\n",
    "data['text'] = data['text'].apply(expand_contractions)\n",
    "\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the special characters\n",
    "data['text'] = data['text'].apply(remove_special_characters)\n",
    "\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase the text\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numeric words to numbers\n",
    "data['text'] = data['text'].apply(remove_numbers)\n",
    "\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove short words (less than 3 characters)\n",
    "data['text'] = data['text'].apply(remove_small_words)\n",
    "\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove names of countries\n",
    "data['text'] = data['text'].apply(remove_countries)\n",
    "\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the days and months\n",
    "data['text'] = data['text'].apply(remove_days_and_months)\n",
    "\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the stopwords\n",
    "data['text'] = data['text'].apply(stopwords)\n",
    "\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the whitespace\n",
    "data['text'] = data['text'].apply(remove_whitespace)\n",
    "\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the data\n",
    "data['text'] = data['text'].apply(lemmatize)\n",
    "\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fake News Word Cloud (After Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud after preprocessing\n",
    "plot_word_cloud(data, 'category', 'text', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above WordCloud makes it evident that trump, washington, military, said, government are trending topics in fake news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real News Word Cloud (After Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud after preprocessing\n",
    "plot_word_cloud(data, 'category', 'text', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, washington, military, trump, said, new, one were the common words in real news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['text']\n",
    "y = data['category']\n",
    "\n",
    "# Split the data into train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring a Vectoriser\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "# 'Fitting' the Vectoriser\n",
    "tfidf_vect_fit = tfidf_vect.fit(X_train)\n",
    "\n",
    "# Creating 'Test' and 'Train' vectorised dataframes\n",
    "tfidf_train = tfidf_vect_fit.transform(X_train)\n",
    "tfidf_test = tfidf_vect_fit.transform(X_test)\n",
    "\n",
    "# Checking, if we did everything alright\n",
    "tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get evaluation scores and plot a confusion matrix\n",
    "def score_model(model, y_test_true, X_test):\n",
    "    \"\"\" A function that returns scores of a model as well as a confusion matrix\"\"\"\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    precision, recall, fscore, train_support = score(y_test_true, y_pred, pos_label=1, average='binary')\n",
    "    print('Precision: {} / Recall: {} / F1-Score: {} / Accuracy: {}'.format(\n",
    "    round(precision, 3), round(recall, 3), round(fscore,3), round(accuracy_score(y_test_true,y_pred), 3)))\n",
    "    \n",
    "    # Create a confusion matrix \n",
    "    cm = confusion_matrix(y_test_true, y_pred)\n",
    "\n",
    "    # Make a Dataframe, of the metrics with classes\n",
    "    class_label = [0, 1]\n",
    "    df_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n",
    "\n",
    "    # Plot the Model\n",
    "    sns.heatmap(df_cm, annot=True, fmt='d')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Simple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.3.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Logistic Regression Algorithm  \n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Fit Algorithm\n",
    "lr.fit(tfidf_train, y_train)\n",
    "\n",
    "score_model(lr, y_test, tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the logistic regression model implies that 98.3% of the time, the model correctly classifies a news article as true or fake.\n",
    "\n",
    "The precision and recall scores are 98.2 % and 98.3% respectively. Implying that out of the total real news, 98.2 % were truly real.\n",
    "\n",
    "112 samples were classified as fake yet they were true. On the other hand, 114 samples were classified as true yet they were false.\n",
    "\n",
    "13181 samples were correctly classified correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.3.2 Random Forest Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Naive Bayes Algorithm# \n",
    "rf = RandomForestClassifier(min_samples_leaf=20, min_samples_split=20, random_state=100)\n",
    "\n",
    "# Fit Algorithm\n",
    "rf = rf.fit(tfidf_train , y_train)\n",
    "\n",
    "score_model(rf, y_test, tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest model performs slightly worse than the logistic regression model with an accuracy score of 98.2%. \n",
    "\n",
    "This implies that 98.2% of the time, a news article is correctly classified as real or fake. \n",
    "\n",
    "The precision and recall scores are 98% and 98.2% respectively. Implying that out of the total real news, 98.1% were truly real. 117 samples were classified as fake yet they were true. \n",
    "\n",
    "On the other hand, 128 samples were classified as true yet they were false. 13177 samples were correctly classified correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Advanced ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Ada Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating an AdaBoost Classifier\n",
    "ada_boost = AdaBoostClassifier()\n",
    "\n",
    "# Fitting the model on the training data\n",
    "ada_boost.fit(tfidf_train, y_train)\n",
    "\n",
    "# scoring the adaboost model\n",
    "score_model(ada_boost, y_test, tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ada Boosting model performs better than the logistic regression and random forest models with an accuracy score of 99.5%. This can be attributed to the fact that it is able to capture more non-linear relationships in the data. This implies that a news article is correctly classified 99.5% of the time. \n",
    "\n",
    "The precision and recall scores are 99.4% and 99.6%. This implies that out of the total real news, 99.4% were truly real.\n",
    "\n",
    "23 samples were classified as fake yet they were true. On the other hand, 39 samples were classified as true yet they were false.\n",
    "\n",
    "13345 samples were correctly classified correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a Gradient Boosting Classifier\n",
    "grad_boost = GradientBoostingClassifier()\n",
    "\n",
    "# Fiiting to train set\n",
    "grad_boost.fit(tfidf_train, y_train)\n",
    "\n",
    "# Scoring the gradient boosting classifier\n",
    "score_model(grad_boost, y_test, tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gradient Boosting model performs slightly better than the Ada Boosting model with an accuracy score of 99.4%. This implies that a news article is correctly classified 99.4% of the time.\n",
    "\n",
    "The precision and recall scores are 99.1 % and 99.7 %. This implies that out of the total real news, 99.1 % were truly real.\n",
    "\n",
    "17 samples were classified as fake yet they were true. On the other hand, 57 samples were classified as true yet they were false.\n",
    "\n",
    "13333 samples were correctly classified correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 XG Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a class of XG Boost\n",
    "xg_boost = XGBClassifier()\n",
    "\n",
    "# Fitting on training data\n",
    "xg_boost.fit(tfidf_train, y_train)\n",
    "\n",
    "# scoring the XG Boost Classifier\n",
    "score_model(xg_boost, y_test, tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XG Boost model performs better than the other models with an accuracy score of 99.7 %. This implies that a news article is correctly classified 99.7 % of the time.\n",
    "\n",
    "The precision and recall scores are 99.8 % and 99.6 % respectively. This implies that out of the total real news, 99.8 % are truly real.\n",
    "\n",
    "14 samples were classified as fake yet they were true. On the other hand, 28 samples were classified as true yet they were false.\n",
    "\n",
    "13365 samples were correctly classified correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a class that generates tpr and fpr for Area under the curve from ROC\n",
    "class get_roc(object):\n",
    "    \"\"\" A function that gets the roc values\"\"\"\n",
    "\n",
    "    def train_rates(model, feature_train, target_train):\n",
    "        \"\"\" A fucntion that gets the true positive rate, false positive rate and the thresholds\"\"\"\n",
    "        \n",
    "        # Calculate the fpr, tpr, and thresholds for the training set\n",
    "        model_y_train_score = model.decision_function(feature_train)\n",
    "        \n",
    "        # Calculate the probability scores of each point in the train set\n",
    "        model_train_fpr, model_train_tpr, model_thresholds = roc_curve(target_train, model_y_train_score)\n",
    "        \n",
    "        # Seaborn's beautiful styling\n",
    "        sns.set_style('darkgrid', {'axes.facecolor': '0.9'})\n",
    "\n",
    "        # ROC curve for training set\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        lw = 2\n",
    "        plt.plot(model_train_fpr, model_train_tpr, color='darkorange',\n",
    "                lw=lw, label='ROC curve')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.yticks([i/20.0 for i in range(21)])\n",
    "        plt.xticks([i/20.0 for i in range(21)])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic (ROC) Curve for Training Set')\n",
    "        plt.legend(loc='lower right')\n",
    "        print('Training AUC: {}'.format(auc(model_train_fpr, model_train_tpr)))\n",
    "        plt.show()\n",
    "        \n",
    "    def test_rates (model, feature_test, target_test):\n",
    "        \"\"\" A fucntion that gets the true positive rate, false positive rate and the thresholds\"\"\"\n",
    "        model_y_test_score = lr.decision_function(feature_test)\n",
    "        \n",
    "        # Calculate the fpr, tpr, and thresholds for the test set\n",
    "        model_test_fpr, model_test_tpr, model_test_thresholds = roc_curve(target_test, model_y_test_score)\n",
    "        \n",
    "        # Seaborn's beautiful styling\n",
    "        sns.set_style('darkgrid', {'axes.facecolor': '0.9'})\n",
    "\n",
    "        # ROC curve for training set\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        lw = 2\n",
    "        plt.plot(model_test_fpr, model_test_tpr, color='darkorange',\n",
    "                lw=lw, label='ROC curve')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.yticks([i/20.0 for i in range(21)])\n",
    "        plt.xticks([i/20.0 for i in range(21)])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic (ROC) Curve for Training Set')\n",
    "        plt.legend(loc='lower right')\n",
    "        print('Testing AUC: {}'.format(auc(model_test_fpr, model_test_tpr)))\n",
    "        plt.show()\n",
    "        \n",
    "    def combined_rates(model, feature_train, feature_test, target_train, target_test):\n",
    "        \"\"\" A function that gets the roc curves of both train and test in one plot\"\"\"\n",
    "        model_y_train_score = model.decision_function(feature_train)\n",
    "        \n",
    "        # Calculate the probability scores of each point in the train set\n",
    "        model_train_fpr, model_train_tpr, model_thresholds = roc_curve(target_train, model_y_train_score)\n",
    "        \n",
    "        # Calculate the probability scores of each point in the test set\n",
    "        model_y_test_score = lr.decision_function(feature_test)\n",
    "        \n",
    "        # Calculate the fpr, tpr, and thresholds for the test set\n",
    "        model_test_fpr, model_test_tpr, model_test_thresholds = roc_curve(target_test, model_y_test_score)\n",
    "        \n",
    "        print('Model Test AUC: {}'.format(auc(model_test_fpr, model_test_tpr)))\n",
    "        print('Model Train AUC: {}'.format(auc(model_train_fpr, model_train_tpr)))\n",
    "        \n",
    "        plt.figure(figsize=(10,8))\n",
    "        lw = 2\n",
    "        \n",
    "        plt.plot(model_test_fpr, model_test_tpr, color='darkorange',\n",
    "         lw=lw, label='Model Test ROC curve')\n",
    "        plt.plot(model_train_fpr, model_train_tpr, color='blue',\n",
    "         lw=lw, label='Model Train ROC curve')\n",
    "        \n",
    "        # Formatting\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.yticks([i/20.0 for i in range(21)])\n",
    "        plt.xticks([i/20.0 for i in range(21)])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        \n",
    "    def get_auc(model, feature_train, feature_test, target_train, target_test):\n",
    "        \"\"\" A function that gets the roc curves of both train and test in one plot\"\"\"\n",
    "        model_y_train_score = model.decision_function(feature_train)\n",
    "        \n",
    "        # Calculate the probability scores of each point in the train set\n",
    "        model_train_fpr, model_train_tpr, model_thresholds = roc_curve(target_train, model_y_train_score)\n",
    "        \n",
    "        # Calculate the probability scores of each point in the test set\n",
    "        model_y_test_score = lr.decision_function(feature_test)\n",
    "        \n",
    "        # Calculate the fpr, tpr, and thresholds for the test set\n",
    "        model_test_fpr, model_test_tpr, model_test_thresholds = roc_curve(target_test, model_y_test_score)\n",
    "        \n",
    "        print('Model Test AUC: {}'.format(auc(model_test_fpr, model_test_tpr)))\n",
    "        print('Model Train AUC: {}'.format(auc(model_train_fpr, model_train_tpr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1 Logistic Regression ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_combined = get_roc.combined_rates(lr, tfidf_train, tfidf_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 Random Forest ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The area under the curve of the random forest is {roc_auc_score(y_test, rf.predict_proba(tfidf_test)[:,1])}\")\n",
    "# Plotting ROC curve for Random Forest\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax = plt.gca()\n",
    "rf_train = RocCurveDisplay.from_estimator(rf, tfidf_train, y_train, ax=ax, alpha=0.8, label ='Random Forest Train Roc Curve')\n",
    "rf_disp = RocCurveDisplay.from_estimator(rf, tfidf_test, y_test, ax=ax, alpha=0.8, label= 'Random Forest Test RoC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.3 Ada Boosting Classifier ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The area under the curve of the Ada Boosting classifier is {roc_auc_score(y_test, ada_boost.predict_proba(tfidf_test)[:,1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting ROC curve for Ada Boosting Classifier\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax = plt.gca()\n",
    "ada_disp = RocCurveDisplay.from_estimator(ada_boost, tfidf_test, y_test, ax=ax, alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.4 Gradient Boosting Classifier ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The area under the curve of the gradient boosting classifier is {roc_auc_score(y_test, grad_boost.predict_proba(tfidf_test)[:,1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting ROC curve for Gradient Boosting Classifier\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax = plt.gca()\n",
    "grad_disp = RocCurveDisplay.from_estimator(grad_boost, tfidf_test, y_test, ax=ax, alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.5 XG Boosting Classifier ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The area under the curve of the XG Boost is {roc_auc_score(y_test, xg_boost.predict_proba(tfidf_test)[:,1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting ROC curve for XG Boosting Classifier\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax = plt.gca()\n",
    "xg_disp = RocCurveDisplay.from_estimator(xg_boost, tfidf_test, y_test, ax=ax, alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class that gets scores\n",
    "class get_scores(object):\n",
    "    \"\"\" a class that gets the scores from a model\"\"\"\n",
    "    def acc(model, x, y):\n",
    "        \"\"\" A function that gets the accuracy score\"\"\"\n",
    "        y_pred = model.predict(x)\n",
    "        score = accuracy_score(y, y_pred)\n",
    "        return score\n",
    "    \n",
    "    def precision (model, x, y):\n",
    "        \"\"\" A function that gets the precision scores\"\"\"\n",
    "        return precision_score(y, model.predict(x))\n",
    "    \n",
    "    def recall(model, x, y):\n",
    "        \"\"\" A function that gets the recall scores\"\"\"\n",
    "        return recall_score(y, model.predict(x))\n",
    "    \n",
    "    def f1 (model, x, y):\n",
    "        \"\"\" A function that that gets the f1 scores\"\"\"\n",
    "        return f1_score(y, model.predict(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating instances of get_scores class\n",
    "lr_scores  = get_scores\n",
    "rf_scores = get_scores\n",
    "gb_score = get_scores\n",
    "ada_score = get_scores\n",
    "xg_score = get_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4.5.1 Train Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train summary table\n",
    "train_summary_table = pd.DataFrame({'Model': [],\n",
    "                              'Accuracy': [], \n",
    "                              'Precision': [], 'Recall': [], 'F1 Score': [],\n",
    "                              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train summary\n",
    "train_summary_table.loc[0] = [\"Logistic Regression\",\n",
    "                         lr_scores.acc(lr, tfidf_train, y_train),\n",
    "                         lr_scores.precision(lr, tfidf_train, y_train),\n",
    "                         lr_scores.recall(lr, tfidf_train, y_train),\n",
    "                         lr_scores.f1(lr, tfidf_train, y_train)]\n",
    "\n",
    "train_summary_table.loc[2] = [\"Random Forest\",\n",
    "                        rf_scores.acc(rf, tfidf_train, y_train),\n",
    "                        rf_scores.precision(rf, tfidf_train, y_train),\n",
    "                        rf_scores.recall(rf, tfidf_train, y_train),\n",
    "                        rf_scores.f1(rf, tfidf_train, y_train)]\n",
    "\n",
    "train_summary_table.loc[2] = [\"Gradient Boost\",\n",
    "                        gb_score.acc(grad_boost, tfidf_train, y_train),\n",
    "                        gb_score.precision(grad_boost, tfidf_train, y_train),\n",
    "                        gb_score.recall(grad_boost, tfidf_train, y_train),\n",
    "                        gb_score.f1(grad_boost, tfidf_train, y_train)]\n",
    "\n",
    "train_summary_table.loc[3] = [\"Ada Boosting Classifier\",\n",
    "                        ada_score.acc(ada_boost, tfidf_train, y_train),\n",
    "                        ada_score.precision(ada_boost, tfidf_train, y_train),\n",
    "                        ada_score.recall(ada_boost, tfidf_train, y_train),\n",
    "                        ada_score.f1(ada_boost, tfidf_train, y_train)]\n",
    "\n",
    "train_summary_table.loc[4] = [\"XG Boosting Classifier\",\n",
    "                        xg_score.acc(xg_boost, tfidf_train, y_train),\n",
    "                        xg_score.precision(xg_boost, tfidf_train, y_train),\n",
    "                        xg_score.recall(xg_boost, tfidf_train, y_train),\n",
    "                        xg_score.f1(xg_boost, tfidf_train, y_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the train summary table\n",
    "train_summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4.5.2 Test Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test summary table\n",
    "#summary table\n",
    "test_summary_table = pd.DataFrame({'Model': [],\n",
    "                              'Accuracy': [], \n",
    "                              'Precision': [], 'Recall': [], 'F1 Score': [],\n",
    "                              })\n",
    "\n",
    "test_summary_table.loc[0] = [\"Logistic Regression\",\n",
    "                         lr_scores.acc(lr, tfidf_test, y_test),\n",
    "                         lr_scores.precision(lr, tfidf_test, y_test),\n",
    "                         lr_scores.recall(lr, tfidf_test, y_test),\n",
    "                         lr_scores.f1(lr, tfidf_test, y_test)]\n",
    "\n",
    "test_summary_table.loc[1] = [\"Random Forest\",\n",
    "                         rf_scores.acc(rf, tfidf_test, y_test),\n",
    "                         rf_scores.precision(rf, tfidf_test, y_test),\n",
    "                         rf_scores.recall(rf, tfidf_test, y_test),\n",
    "                         rf_scores.f1(rf, tfidf_test, y_test)]\n",
    "\n",
    "test_summary_table.loc[2] = [\"Gradient Boost\",\n",
    "                        gb_score.acc(grad_boost, tfidf_test, y_test),\n",
    "                        gb_score.precision(grad_boost, tfidf_test, y_test),\n",
    "                        gb_score.recall(grad_boost, tfidf_test, y_test),\n",
    "                        gb_score.f1(grad_boost, tfidf_test, y_test)]\n",
    "\n",
    "test_summary_table.loc[3] = [\"Ada Boosting Classifier\",\n",
    "                        ada_score.acc(ada_boost, tfidf_test, y_test),\n",
    "                        ada_score.precision(ada_boost, tfidf_test, y_test),\n",
    "                        ada_score.recall(ada_boost, tfidf_test, y_test),\n",
    "                        ada_score.f1(ada_boost, tfidf_test, y_test)]\n",
    "\n",
    "test_summary_table.loc[4] = [\"XG Boosting Classifier\",\n",
    "                        xg_score.acc(xg_boost, tfidf_test, y_test),\n",
    "                        xg_score.precision(xg_boost, tfidf_test, y_test),\n",
    "                        xg_score.recall(xg_boost, tfidf_test, y_test),\n",
    "                        xg_score.f1(xg_boost, tfidf_test, y_test)]\n",
    "\n",
    "# showing the test summary table\n",
    "test_summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a class that performs a cross validation score\n",
    "class cross_v_scores(object):\n",
    "    \"\"\" A class that performs cross validation\"\"\"\n",
    "    def acc_score(model, x, y):\n",
    "        \"\"\" A function that gets the mean cross validation accuracy\"\"\"\n",
    "        mean_score = np.mean(cross_val_score(model, x, y, cv=3, scoring='accuracy'))\n",
    "        return mean_score\n",
    "    \n",
    "    def mse(model, x, y):\n",
    "        \"\"\" A function that calculates the mean squared error\"\"\"\n",
    "        mean_mse = np.mean(-cross_val_score(model, x, y, cv=5, scoring=\"neg_mean_squared_error\"))\n",
    "        return mean_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cv = cross_v_scores\n",
    "rf_cv = cross_v_scores\n",
    "ada_cv = cross_v_scores\n",
    "grad_cv = cross_v_scores\n",
    "xg_cv = cross_v_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a cross validation summary table\n",
    "cross_validation_summary_table = pd.DataFrame({'Model': [],\n",
    "                              'Accuracy': [], \n",
    "                              'mean squared error': [],\n",
    "                              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "cross_validation_summary_table.loc[0] = [\"Logistic Regression\",\n",
    "                                         lr_cv.acc_score(lr, tfidf_test, y_test),\n",
    "                                         lr_cv.mse(lr, tfidf_test, y_test)]\n",
    "\n",
    "# random forest\n",
    "cross_validation_summary_table.loc[1] = [\"Random Forest\",\n",
    "                                         rf_cv.acc_score(rf, tfidf_test, y_test),\n",
    "                                         rf_cv.mse(rf, tfidf_test, y_test)]\n",
    "\n",
    "\n",
    "# XG Boost\n",
    "cross_validation_summary_table.loc[4] = [\"XG Boosting Classifier\",\n",
    "                                         xg_cv.acc_score(xg_boost, tfidf_test, y_test),\n",
    "                                         xg_cv.mse(xg_boost, tfidf_test, y_test)]\n",
    "\n",
    "# showing the table\n",
    "cross_validation_summary_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluated 5 different models in this project. 2 of which were simple algorithms: Logistic Regression and Random Forrest, and 3 being advanced algorithms: Ada Boosting Classifier, Gradient Boosting Classifier, and Extreme Gradient Boosting Classifier.\n",
    "\n",
    "The accuracy and precision scores were compared above using the summary tables. Of the 5 algorithms, XGBoost returned the best accuracy of 99.7% on our test data.\n",
    "\n",
    "With this extremely high accuracy, we chose to employ cross validation in an aim to reduce the potential overfitting. XGBoost Classifier averaged the the best accuracy (99.5%) as well as the least mean squared error (0.005). Therefore, we saw this as the best choice to adopt for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Deployment\n",
    "> In this section, we shall be building a pipeline with the text vectorizer as well as the XGBoosting Classifier. This pipeline will be important when it comes to building our ml-app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "pipe = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                 ('xgb', XGBClassifier())])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "pipe.predict(X_test)\n",
    "\n",
    "# Pickle the pipeline\n",
    "pickle.dump(pipe, open('./models/model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Findings\n",
    "- The common keywords in fake news are administration, trump, government, republican\n",
    "\n",
    "- Fake news is fairly distributed throughout the year but periods following elections , campaigns, new governments see a slight surge in volume\n",
    "\n",
    "- The most prevalent topics in fake news are politics(evidenced by common words in the word cloud like Republican, trump, obama, government) and war indicated by conflict, attack, terrorist\n",
    "\n",
    "- Trump is the name that appears most in fake news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Limitations\n",
    "- The data was only restricted to a timeframe of 2015-2018\n",
    "\n",
    "- Most of the news articles covered USA leaving out other locations\n",
    "\n",
    "- Our algorithms were computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Future Work\n",
    "- Build an automated fact-checking system that combines data looking at different aspects to help non-experts in classifying news.\n",
    "\n",
    "- Use data that covers a wide range of time focusing on world news.\n",
    "\n",
    "- Use PySpark to process data so as to reduce computation complexity\n",
    "\n",
    "- Use twitter API to get current news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.Conclusion\n",
    "- Every single news has different characteristics so there is a need for a system that can check the content of the news in depth.\n",
    "\n",
    "- The results suggested that the approach is highly favorable since the application helps in classifying fake news and identifying key features that can be used for fake news detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.Reccommendations\n",
    "- The use of the system classifier to detect whether an article posted is legitimate to avoid misinformation to the readers\n",
    "- The classifier can be used to improve the accuracy and effectiveness of other fake news detection tools and systems\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0fa7090e1727e2870c3dbf102c28a851b11d162f017b7d53e6152c79c8e038dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
